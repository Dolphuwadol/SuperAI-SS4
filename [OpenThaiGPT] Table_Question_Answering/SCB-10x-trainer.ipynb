{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SCB-10-x-Trainer-LLM","metadata":{"id":"IxBt9NAW-lMV"}},{"cell_type":"markdown","source":"https://github.com/scb-10x/sft-trainer-example","metadata":{"id":"muKv_DOg-ju_"}},{"cell_type":"code","source":"!git clone https://github.com/scb-10x/sft-trainer-example","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_oYGNb1y2US","outputId":"c1af5484-87fc-4a16-bb19-4935724c4f95","execution":{"iopub.status.busy":"2024-04-29T08:49:21.988137Z","iopub.execute_input":"2024-04-29T08:49:21.988445Z","iopub.status.idle":"2024-04-29T08:49:23.420324Z","shell.execute_reply.started":"2024-04-29T08:49:21.988419Z","shell.execute_reply":"2024-04-29T08:49:23.419021Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'sft-trainer-example'...\nremote: Enumerating objects: 50, done.\u001b[K\nremote: Counting objects: 100% (50/50), done.\u001b[K\nremote: Compressing objects: 100% (34/34), done.\u001b[K\nremote: Total 50 (delta 22), reused 41 (delta 13), pack-reused 0\u001b[K\nUnpacking objects: 100% (50/50), 9.07 KiB | 774.00 KiB/s, done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -r \"/kaggle/working/sft-trainer-example/requirements.txt\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhR9XjR6z_NN","outputId":"62ca0823-93c9-484e-d6b6-89da89afb4d4","execution":{"iopub.status.busy":"2024-04-29T08:49:49.837571Z","iopub.execute_input":"2024-04-29T08:49:49.838371Z","iopub.status.idle":"2024-04-29T08:50:20.020758Z","shell.execute_reply.started":"2024-04-29T08:49:49.838337Z","shell.execute_reply":"2024-04-29T08:50:20.019784Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes==0.43.1 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 1))\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nCollecting trl==0.8.6 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 2))\n  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\nCollecting transformers==4.40.1 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 3))\n  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.29.3 in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 4)) (0.29.3)\nCollecting datasets==2.19.0 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 5))\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\nCollecting evaluate==0.4.1 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 6))\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting peft==0.10.0 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 7))\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nCollecting openai==1.23.3 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 8))\n  Downloading openai-1.23.3-py3-none-any.whl.metadata (21 kB)\nCollecting sacrebleu==2.4.2 (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 9))\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 10)) (0.2.0)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/sft-trainer-example/requirements.txt (line 11)) (2.6.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (1.26.4)\nCollecting tyro>=0.5.11 (from trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2))\n  Downloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3))\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 4)) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (3.9.1)\nCollecting responses<0.19 (from evaluate==0.4.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 6))\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (1.3.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (4.9.0)\nCollecting portalocker (from sacrebleu==2.4.2->-r /kaggle/working/sft-trainer-example/requirements.txt (line 9))\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.4.2->-r /kaggle/working/sft-trainer-example/requirements.txt (line 9)) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.4.2->-r /kaggle/working/sft-trainer-example/requirements.txt (line 9)) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu==2.4.2->-r /kaggle/working/sft-trainer-example/requirements.txt (line 9)) (5.2.1)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX->-r /kaggle/working/sft-trainer-example/requirements.txt (line 11)) (3.20.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (4.0.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (0.14.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.23.3->-r /kaggle/working/sft-trainer-example/requirements.txt (line 8)) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 3)) (1.26.18)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2)) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2)) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2))\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0->-r /kaggle/working/sft-trainer-example/requirements.txt (line 5)) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2)) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.43.1->-r /kaggle/working/sft-trainer-example/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r /kaggle/working/sft-trainer-example/requirements.txt (line 2)) (0.1.2)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.23.3-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, portalocker, sacrebleu, responses, tyro, tokenizers, openai, bitsandbytes, transformers, datasets, trl, peft, evaluate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.0\n    Uninstalling transformers-4.30.0:\n      Successfully uninstalled transformers-4.30.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.18.0\n    Uninstalling datasets-2.18.0:\n      Successfully uninstalled datasets-2.18.0\nSuccessfully installed bitsandbytes-0.43.1 datasets-2.19.0 evaluate-0.4.1 openai-1.23.3 peft-0.10.0 portalocker-2.8.2 responses-0.18.0 sacrebleu-2.4.2 shtab-1.7.1 tokenizers-0.19.1 transformers-4.40.1 trl-0.8.6 tyro-0.8.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install transformers==4.30","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajlF3Xlk9Tsq","outputId":"10db7fba-5d90-4e82-fbd9-696261506029","execution":{"iopub.status.busy":"2024-04-29T08:50:39.492597Z","iopub.execute_input":"2024-04-29T08:50:39.492970Z","iopub.status.idle":"2024-04-29T08:50:52.017986Z","shell.execute_reply.started":"2024-04-29T08:50:39.492932Z","shell.execute_reply":"2024-04-29T08:50:52.016834Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.30 in /opt/conda/lib/python3.10/site-packages (4.30.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (2024.2.2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Generate_dataset","metadata":{"id":"FrYzSVQ_zMNm"}},{"cell_type":"code","source":"import json\nimport os\nfrom datasets import load_dataset\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\ndef call_openai(\n    user_prompt: str,\n    model=\"typhoon-instruct\",\n    max_tokens=1000,\n    top_p=0.1,\n    temperature=1.0,\n):\n    client = OpenAI(base_url=os.environ['OPENAI_BASE_URL'], api_key=os.environ['OPENAI_API_KEY'])\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        top_p=top_p,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    content = response.choices[0].message.content\n    finish_reason = response.choices[0].finish_reason\n    return content\n\n\ndef translate_task(text: str, outpath: str):\n    assert isinstance(text, str)\n    user_prompt = f\"\"\"\nTranslate message below to Thai:\n---\n{text}\n---\nOutput only translation result\n\"\"\"\n    translate_resp = call_openai(user_prompt)\n    row = {\"en\": text, \"th\": translate_resp}\n    with open(outpath, \"a\") as w:\n        w.write(f\"{json.dumps(row, ensure_ascii=False)}\\n\")\n\n\ndef process_row(example, outpath):\n    for conv in example[\"conversations\"]:\n        translate_task(conv['value'], outpath=outpath)\n\n\ndef main():\n    ds = load_dataset(\"openaccess-ai-collective/oasst1-guanaco-extended-sharegpt\", split=\"train\")\n    ds = ds.select(range(100))\n    print(ds)\n    for row in tqdm(iter(ds)):\n        process_row(row, outpath=\"output.jsonl\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"id":"TEy9jKBOzOti"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train","metadata":{"id":"YH451cLJzVqp"}},{"cell_type":"code","source":"from dataclasses import dataclass, field\nfrom typing import Optional\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nimport torch\nimport os\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    Define the arguments used in this script.\n    \"\"\"\n\n    model_name: Optional[str] = field(default=\"scb10x/typhoon-7b\", metadata={\"help\": \"the model name\"})\n    dataset_name: Optional[str] = field(default='output.jsonl', metadata={\"help\": \"the dataset name\"})\n    use_4_bit: Optional[bool] = field(default=True, metadata={\"help\": \"use 4 bit precision\"})\n    batch_size: Optional[int] = field(default=4, metadata={\"help\": \"input batch size\"})\n    lr: Optional[float] = field(default=4e-4, metadata={\"help\": \"learning rate\"})\n    gradient_accumulation_steps: Optional[int] = field(default=1, metadata={\"help\": \"input grad accum step\"})\n    max_seq_length: Optional[int] = field(default=2048, metadata={\"help\": \"max sequence length\"})\n    output_dir: Optional[str] = field(default=\"ckpt\", metadata={\"help\": \"ckpt output\"})\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    torch_dtype = torch.bfloat16\n    # Load model and tokenizer\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        llm_int8_threshold=6.0,\n        llm_int8_has_fp16_weight=False,\n        bnb_4bit_compute_dtype=torch_dtype,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_use_double_quant=True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(args.model_name, quantization_config=bnb_config, torch_dtype=torch_dtype, attn_implementation=\"flash_attention_2\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    peft_config = LoraConfig(\n        r=32,\n        lora_alpha=8,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    tokenizer.pad_token_id = tokenizer.unk_token_id\n    # Update the model config to use the new eos & bos token\n    if getattr(model, \"config\", None) is not None:\n        model.config.pad_token_id = tokenizer.pad_token_id\n        model.config.bos_token_id = tokenizer.bos_token_id\n        model.config.eos_token_id = tokenizer.eos_token_id\n    if getattr(model, \"generation_config\", None) is not None:\n        model.generation_config.bos_token_id = tokenizer.bos_token_id\n        model.generation_config.eos_token_id = tokenizer.eos_token_id\n        model.generation_config.pad_token_id = tokenizer.pad_token_id\n\n    if os.path.exists(args.dataset_name):\n        dataset = load_dataset('json', data_files=args.dataset_name)['train']\n    else:\n        dataset = load_dataset(args.dataset_name, split=\"train\")\n\n    def formatting_prompts_func(examples):\n        INPUT_COLUMN = \"en\"\n        OUTPUT_COLUMN = \"th\"\n        output_texts = []\n        for i in range(len(examples[INPUT_COLUMN])):\n            input = examples[INPUT_COLUMN][i]\n            output = examples[OUTPUT_COLUMN][i]\n            text = f'''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\n### Instruction:\nTranslate message to Thai\n### Input:\n{input}\n### Response:\n{output}{tokenizer.eos_token}''' # <-- make sure there are eos_token in the format_prompt; sfttrainer doesn't add eos token internally.\n            output_texts.append(text)\n        return output_texts\n\n    # we need to make sure it\n    response_template = \"\\n### Response:\"\n    response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n    print(args)\n    training_arguments = TrainingArguments(\n        per_device_train_batch_size=args.batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        per_device_eval_batch_size=args.batch_size,\n        report_to=['tensorboard'],\n        optim='adamw_torch',\n        learning_rate=args.lr,\n        logging_steps=1,\n        bf16=True,\n        fp16=False,\n        save_steps=1,\n        save_strategy='epoch',\n        gradient_checkpointing=True,\n        output_dir=args.output_dir\n    )\n\n    collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n    trainer = SFTTrainer(\n        model,\n        args=training_arguments,\n        data_collator=collator,\n        tokenizer=tokenizer,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        max_seq_length=args.max_seq_length,\n        formatting_func=formatting_prompts_func\n    )\n\n    trainer.train()\n    trainer.save_model(args.output_dir)\n\nif __name__ == '__main__':\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"fNcyZjjazWZp","outputId":"43ec445b-ca8f-4665-a837-95892e1489a2"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Some specified arguments are not used by the HfArgumentParser: ['-f', '/root/.local/share/jupyter/runtime/kernel-43ac8e06-ff8c-45a4-9b2e-bafe09c0ca78.json']","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-3c7b89210302>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-3c7b89210302>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScriptArguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args_into_dataclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtorch_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Load model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/hf_argparser.py\u001b[0m in \u001b[0;36mparse_args_into_dataclasses\u001b[0;34m(self, args, return_remaining_strings, look_for_args_file, args_filename, args_file_flag)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Some specified arguments are not used by the HfArgumentParser: ['-f', '/root/.local/share/jupyter/runtime/kernel-43ac8e06-ff8c-45a4-9b2e-bafe09c0ca78.json']"]}],"execution_count":12},{"cell_type":"markdown","source":"## Evaluate","metadata":{"id":"5ecvoT7szIXI"}},{"cell_type":"code","source":"import json\nimport os\nfrom datasets import load_dataset\nfrom sacrebleu.metrics import BLEU\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nfrom tqdm import tqdm\nimport argparse\nbleu = BLEU(tokenize=\"flores200\")\n\n\ndef get_prompt(input: str):\n    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\n### Instruction:\nTranslate message to Thai\n### Input:\n{input}\n### Response:\"\"\"\n    return prompt\n\n\ndef main(base_model: str, lora_path: str, eval_dataset):\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    device = torch.device(\"cuda\")\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=False,\n        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    )\n    print(f\"loaded: {base_model}\")\n    model = PeftModel.from_pretrained(model, lora_path)\n    model.eval()\n    model.to(device)\n    if os.path.exists(eval_dataset):\n        ds = load_dataset(\"json\", data_files={\"validation\": eval_dataset}, split=\"validation\")\n    else:\n        ds = load_dataset(eval_dataset, split=\"validation\")\n    results = []\n    references = []\n    inputs = []\n\n    for row in tqdm(iter(ds), total=len(ds)):\n        prompt = get_prompt(row[\"en\"])\n        references.append(row[\"th\"])\n        inputs.append(row['en'])\n        input = tokenizer([prompt], return_tensors=\"pt\").to(device)\n        output = model.generate(**input, max_new_tokens=256)\n        output = tokenizer.decode(output[0][input['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n        results.append(output)\n\n    print(\n        {\n            \"bleu\": str(bleu.corpus_score(results, [references])),\n        }\n    )\n    with open(\"eval_results.json\", \"w\") as w:\n        json.dump(\n            [{\"pred\": pred, \"ref\": ref, \"input\": ip} for pred, ref, ip in zip(results, references, inputs)], w, ensure_ascii=False\n        )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--base-model', type=str, default='scb10x/typhoon-7b')\n    parser.add_argument('--lora-path', type=str)\n    parser.add_argument ('--eval-dataset', type=str, default='scb_mt_enth_2020_wiki_1k_test.jsonl')\n    args = parser.parse_args()\n    main(\n        args.base_model,\n        lora_path=args.lora_path,\n        eval_dataset=args.eval_dataset,\n    )\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"MOxpI0-7y-E2","outputId":"a4e9ab68-f6f3-449b-9731-fc84bb9294c6"},"outputs":[{"output_type":"stream","name":"stderr","text":"usage: colab_kernel_launcher.py [-h] [--base-model BASE_MODEL] [--lora-path LORA_PATH]\n\n                                [--eval-dataset EVAL_DATASET]\n\ncolab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-43ac8e06-ff8c-45a4-9b2e-bafe09c0ca78.json\n"},{"output_type":"error","ename":"SystemExit","evalue":"2","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]}],"execution_count":11},{"cell_type":"code","source":"!python /kaggle/working/sft-trainer-example/train.py --dataset_name scb10x/scb_mt_enth_2020_aqdf_1k --gradient_accumulation_steps 4","metadata":{"id":"_AKEopoK7zGv","execution":{"iopub.status.busy":"2024-04-29T08:51:50.236515Z","iopub.execute_input":"2024-04-29T08:51:50.237432Z","iopub.status.idle":"2024-04-29T08:52:09.420031Z","shell.execute_reply.started":"2024-04-29T08:51:50.237382Z","shell.execute_reply":"2024-04-29T08:52:09.418913Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2024-04-29 08:51:57.335033: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-29 08:51:57.335145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-29 08:51:57.438407: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|█████████████████████████████| 595/595 [00:00<00:00, 3.24MB/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/sft-trainer-example/train.py\", line 116, in <module>\n    main()\n  File \"/kaggle/working/sft-trainer-example/train.py\", line 38, in main\n    model = AutoModelForCausalLM.from_pretrained(args.model_name, quantization_config=bnb_config, torch_dtype=torch_dtype, attn_implementation=\"flash_attention_2\")\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 456, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 957, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 671, in __getitem__\n    raise KeyError(key)\nKeyError: 'mistral'\n","output_type":"stream"}],"execution_count":8}]}